---
title: "Regressão por árvores de decisão e floresta aleatória"
author: "Robson Bruno Dutra Pereira"
format: html
editor: visual
---

## Regressão por árvores de decisão e floresta aleatória

### Árvores de regressão

As árvores de decisão foram inicialmente propostas para problemas de classificação, porém, podem ser adaptadas de forma simples para problemas de regressão. A regressão por árvores de decisão ou simplesmente as árvores de regressão envolvem dividir o espaço preditor em várias regiões retangulares simples tomando como predição a média ou outra medida simples das observações de treinamento da região. O conjunto de regras de divisão usado para segmentar o espaço preditor pode ser resumido em um diagrama semelhante a uma árvore, conforme a Figura a seguir.

```{r}
#| message: false
#| warning: false
#| echo: false
data(Hitters, package = "ISLR")
dados <- na.omit(Hitters)
X <- model.matrix(Salary~., dados)[,-1]
y <- dados$y # resposta
dados2 <- data.frame(dados[,c(5,6,19)])
colnames(dados2) <- c("x1", "x2", "y")
set.seed(1)
tr <- round(0.5*nrow(X))
treino <- sample(1:nrow(X), tr, replace = F)
library(tree)
tree1 <- tree(y ~ x1 + x2, dados2, subset = treino)
cv.tree1 <- cv.tree(tree1)
prune1 <- prune.tree(tree1, best = 4)
plot(prune1)
text(prune1, pretty = 0)
```

O gráfico a seguir expõe o modelo obtido em função das duas variáveis preditoras, podendo-se observar que o valor previsto é constante em cada região retangular correspondente ao diagrama ilustrado anteriormente.

```{r}
#| message: false
#| warning: false
#| echo: false

xs <- seq(min(dados2$x1), max(dados2$x1), length = 30)
ys <- seq(min(dados2$x2), max(dados2$x2), length = 30)
xys <- expand.grid(xs,ys)
colnames(xys) <- c("x1", "x2")
zs <- matrix(predict(prune1 , xys), nrow = length(xs))

n.cols <- 200
palette <- colorRampPalette(c("palevioletred", "lightseagreen"))(n.cols)
zfacet <- zs[-1, -1] + zs[-1, -10] + zs[-10, -1] + zs[-10, -10]
facetcol <- cut(zfacet, n.cols)

persp(x=xs, y=ys, z=zs, theta=-45, phi=30, 
            xlab="x1", ylab="x2", zlab="y", 
            col = palette[facetcol])
```

Pode-se observar o mesmo modelo de árvore de regressão de forma bidimensional, ficando mais clara a divisão do espaço preditor.

```{r}
#| message: false
#| warning: false
#| echo: false

library(ggplot2)
xys$y <- predict(prune1 , xys)
ggplot(xys, aes(y = x2, x = x1)) + 
  geom_raster(aes(fill = y)) +
  labs(col = "y") + 
  scale_fill_gradientn(colors = c ("palevioletred", "lightseagreen")) + 
  geom_segment(aes(x = 0, y = 37.55, xend = 120, yend = 37.55), 
               lty = 2, cex = 1, col = "navy") +
  geom_segment(aes(x = 0, y = 60, xend = 120, yend = 60), 
               lty = 2, cex = 1, col = "navy") +
  geom_segment(aes(x = 0, y = 60, xend = 120, yend = 60), 
               lty = 2, cex = 1, col = "navy") +
  geom_segment(aes(x = 65, y = 60, xend = 65, yend = 106), 
               lty = 2, cex = 1, col = "navy") +
  theme_bw()
```

Sejam $k$ variáveis de entrada e uma resposta, ou seja, $(\mathbf{x}_i,y_i)$, com $\mathbf{x} = (x_{i1}, x_{i2},..., x_{iK})$, para $i=1,...,N$ observações de treino, o algoritmo de (*Classification and Regression Trees* - CART) para regressão define a cada iteração a variável preditora e o seu nível para particionar o espaço dos preditores. Considerando $J$ regiões $R_1, R_2, ..., R_J$, o valor predito será uma constante $\gamma_j$, em cada região, $j=1,...,J$. Portanto, o modelo de árvore de regressão pode ser definido conforme a Equação a seguir, $\{R_j,\gamma_j\}, j=1,\ldots,J$, onde $I(\mathbf{x} \in R_j)$ é uma função indicativa que recebe 1 se $\mathbf{x}$ pertence à região $R_j$ e 0 caso contrário. O melhor $\gamma_j$ para minimizar a soma dos quadrados é a média das observações na região, $\hat{\gamma}_j = (\bar{y}_i | \mathbf{x}_i \in R_j$).

$$
\begin{aligned}
f(\mathbf{x}) = T(\mathbf{x},R_j,\gamma_j) = \sum_{j=1}^J \gamma_jI(\mathbf{x} \in R_j)
\end{aligned}
$$

Considerando todos os dados de treinamento, as divisões são definidas tomando uma variável para divisão, $x_k$, $k = 1,..., K$, e um ponto de divisão $x_k = s$, $R_1(k,s)$ = ${\mathbf{x}|x_k \leq s}$ e $R_2(k,s)$ = ${\mathbf{x}|x_k>s}$. Portanto, o algoritmo CART busca a variável para o particionamento e o valor desta na divisão resolvendo:

$$
\begin{aligned}
    \min_{k,s} \Bigl[ \min_{\gamma_1} \sum_{x_i \in R_1(k,s)} (y_i - \gamma_1)^2 +  \min_{\gamma_2} \sum_{x_i \in R_2(k,s)} (y_i - \gamma_2)^2 \Bigr].
\end{aligned}
$$

O algoritmo CART repete o particionamento recursivamente até um determinado critério de parada ser alcançado, por exemplo, até um número mínimo de observações em cada partição ser atingido.

Seja um conjunto de dados da liga maior americana de Baseball para as temporadas de 1986 e 1987. São disponibilizadas 322 observações de jogadores da liga, incluindo número de batidas, número de corridas, tempo em anos na liga, etc. A variável de interesse a ser predita é o salário do jogador. A seguir é plotado um gráfico de correlação entra tais variáveis.

```{r}
#| message: false
#| warning: false
#| echo: false

data(Hitters, package = "ISLR")
dados <- na.omit(Hitters)
r <- cor(dados[,-c(14,15,20)])
library(corrplot)
corrplot::corrplot(r, method="color", 
                   type="upper", order="hclust", 
                   addCoef.col = NULL, tl.srt=45, 
                   diag=FALSE)
```

Seja um modelo de árvore de regressão para prever o salário em função das demais variáveis considerando metade dos dados selecionados aleatoriamente para treino do modelo.

```{r}
#| message: false
#| warning: false
#| echo: false
tr <- round(0.5*nrow(dados))
treino <- sample(1:nrow(dados), tr, replace = F)
library(tree)
tree1 <- tree(Salary ~ ., dados, subset = treino)
tree1
```

O diagrama do modelo pode ser plotado conforme segue. Acima tem-se `CRBI<307.5` que consiste na primeira partição ou nó raiz. Qualquer subconjunto de partições interligadas da árvore pode ser chamado de sub-árvore. Cada partição gera duas regiões às quais podem ou não ser mais particionadas. As partições finais são chamadas de nós terminais ou folhas. Neste caso tem-se 10 nós terminais, ou seja, 10 valores previstos distintos para 10 regiões distintas do espaço de preditores. Por exemplo, tomando as seguintes partições `CRuns < 325.5 82  6125000  296.3` e `CRuns > 208.5 22   514100  498.8 *`, tem-se o valor previsto do salário do jogador igual a 498.8 unidades monetárias com soma dos quadrados dos erros igual a 514100.

```{r}
#| message: false
#| warning: false
#| echo: false
plot(tree1)
text(tree1, pretty = 0, cex = .7)
```

Pode-se realizar uma validação cruzada para "podar" ou "secar" a árvore com a finalidade de diminuir o sobreajuste.

```{r}
#| message: false
#| warning: false
#| echo: false
set.seed(3)
cv.tree1 <- cv.tree(tree1)
plot(cv.tree1$size, cv.tree1$dev, type = "b", col = "blue")
```

A árvore podada plotada apresenta apenas 4 folhas, com estrutura mais simples, facilitando a interpretação e buscando melhor generalização.

```{r}
#| message: false
#| warning: false
#| echo: false
prune1 <- prune.tree(tree1, best = 4)
prune1
```

```{r}
#| message: false
#| warning: false
#| echo: false
plot(prune1)
text(prune1, pretty = 0)
```

### *Bagged trees ou bagging*

*Bagged trees ou bagging* ou, em português, "árvores ensacadas" é um método baseado em árvore de decisão que pode ser aplicado tanto para regressão quanto para classificação. O método consiste em agregar várias árvores de decisão as quais são estimadas a partir de reamostragem por reposição dos dados de treino. Ou seja, antes de estimar cada árvore, realiza-se um sorteio por reposição dos dados de treino, procedimento este chamado de bootstrap e, posteriormente estima-se a árvore para cada amostra "manipulada". O modelo final é a média de todas as árvores obtidas via bootstrap. Geralmente um número alto de reamostragens é realizado e, portanto, um número alto de árvores é obtido, resultando em um modelo com maior flexibilidade, porém mais difícil de interpretar. O nome *bagging* vem de *bootstrap aggregated* ou agregação por bootstrap.

Considere um conjunto de dados com $N = 10$ observações, com três preditores e uma variável independente, exibido integralmente a seguir.

```{r}
#| message: false
#| warning: false
#| echo: false
library(gt)

set.seed(8)
x1 <- rnorm(10)
x2 <- rnorm(10, 2)
x3 <- rnorm(10, 3)
y <- x1+x2+x3+rnorm(10,1,0.2)

dat <- round(data.frame(i = 1:10, x1,x2,x3,y),2)

dat |> gt() |>
  data_color(
    columns = i,
    target_column = everything(),
    method = "numeric")

```

Um *bootstrap* deste conjunto de dados pode resultar na reamostragem exibida a seguir. Pode-se observar que as observações 4, 5, 6 e 9 foram sorteadas duas vezes, enquanto as observações 1, 3, 7 e 8 não foram sorteadas.

```{r}
#| message: false
#| warning: false
#| echo: false
library(gt)

dat1 <- dat[sample(1:10, 10, T),]

dat1 |> gt() |>
  data_color(
    columns = i,
    target_column = everything(),
    method = "numeric")

```

Abaixo é exibido o diagrama de uma árvore de decisão para a amostra obtida via *bootstrap*. O *bagging* repete este processso diversas vezes, reamostrando os dados por *bootstrap* e estimando para cada "nova" amostra uma nova árvore. Ao final é tomada como predição a média de todas as árvores.

```{r}
#| message: false
#| warning: false
#| echo: false
tr1 <- tree(y ~ .-i, dat1)
plot(tr1)
text(tr1, pretty = 0)
```

Para o mesmo conjunto de dados, sejam 6 árvores obtidas após reamostragem dos dados via *bootstrap* exibidas a seguir.

```{r}
#| message: false
#| warning: false
#| echo: false

x10 <- rnorm(1)
x20 <- rnorm(1, 2)
x30 <- rnorm(1, 3)

x0 <- round(data.frame(x1=x10,x2=x20,x3=x30),2)

pred_tree <- numeric(6)
par(mfrow=c(2,3))

set.seed(11)
for (i in 1:6){
  dat1 <- dat[sample(1:10, 10, T),]
  tr1 <- tree(y ~ .-i, dat1)
  plot(tr1)
  text(tr1, pretty = 0, col = i)
  pred_tree[i] <- predict(tr1, newdata = x0)
  }
```

Seja uma observação futura arbitrária com valores exibidos à seguir:

```{r}
#| message: false
#| warning: false
#| echo: false
x0 |> gt()
```

Considerando as 6 árvores obtidas anteriormente, as previsãos destas para tal observação são: 7.302, 5.730, 6.732, 7.680, 7.680 e 5.490. O modelo *bagging* retorna como resultado a média das previsões das árvores, isto é, $\hat{y} = \frac{1}{6}\sum_{i=b}^6 T(\mathbf{x}_0)$ = 6.769.

```{r}
#| message: false
#| warning: false
#| echo: false
# pred_tree
# mean(pred_tree)
# 7.302 5.730 6.732 7.680 7.680 5.490
# 6.769
```


Para o conjunto de dados Hitters seja um modelo de regressão por *Bagging* considerando duas variáveis, `Cruns` e `RBI`. O modelo plotado é muito mais complexo e pretende apresentar uma melhor acuracidade que um modelo baseado em árvore por justamente congregar diversas árvores baseadas na reamostragem dos dados de treino. Porém, Como tais modelos apresentam maior variância que os modelos de árvore para regressão, é importante avaliar se há sobreajuste.

```{r}
#| message: false
#| warning: false
#| echo: false
#| fig-width: 8
#| fig-height: 8
# Plotando superficie do modelo bagged tree 1
library(randomForest)
bag <- randomForest(Salary ~ CRuns+RBI, dados, subset = treino, mtry = 2,
                    importance = TRUE, ntree = 500)

xs <- seq(min(dados$CRuns), max(dados$CRuns), length = 40)
ys <- seq(min(dados$RBI), max(dados$RBI), length = 40)
xys <- expand.grid(xs,ys)
colnames(xys) <- c("CRuns", "RBI")

zs <- matrix(predict(bag, xys), nrow = length(xs))

n.cols <- 100
palette <- colorRampPalette(c("palevioletred", "lightseagreen"))(n.cols)
zfacet <- zs[-1, -1] + zs[-1, -20] + zs[-20, -1] + zs[-20, -20]
facetcol <- cut(zfacet, n.cols)

persp(x=xs, y=ys, z=zs, theta=-40, phi=30, ticktype='detailed', 
            xlab="CRuns", ylab="RBI", zlab="Salary", 
            col = palette[facetcol])
```

### Floresta aleatória

O modelo de floresta aleatória consiste em uma evolução do modelo *bagging* com a finalidade de diminuir a variância destes. Basicamente, em casos onde há multicolineariedade ou redundância entre as variáves regressoras, o *bagging* pode acarretar na seleção de apenas alguns dos preditores disponíveis durante o particionamento binário recursivo, de forma que as árvores geradas considerando a reamostragem das observações de treino possam apresentar alta correlação entre si, acarretando em alta variabilidade das previsões finais. No modelo de floresta aleatória, antes de cada partição, são selecionadas aleatoriamente $m$ variáveis regressoras, $m<k$, às quais serão consideradas no processo de particionamento binário. Geralmente para regressão recomenda-se $m=k/3$.

A seguir apresenta-se o gráfico de superfície de um modelo de floresta aleatória para o conjunto de dados Hitters considerando duas variáveis, $k=2$, `Cruns` e `RBI`. Para cada particionamento considerou-se $m=2/3 \simeq 1$.

```{r}
#| message: false
#| warning: false
#| echo: false
#| fig-width: 8
#| fig-height: 8
# Plotando superficie do modelo bagged tree 1
library(randomForest)
rf <- randomForest(Salary ~ CRuns+RBI, dados, subset = treino, mtry = 1,
                    importance = TRUE, ntree = 500)

xs <- seq(min(dados$CRuns), max(dados$CRuns), length = 40)
ys <- seq(min(dados$RBI), max(dados$RBI), length = 40)
xys <- expand.grid(xs,ys)
colnames(xys) <- c("CRuns", "RBI")

zs <- matrix(predict(rf, xys), nrow = length(xs))

n.cols <- 100
palette <- colorRampPalette(c("palevioletred", "lightseagreen"))(n.cols)
zfacet <- zs[-1, -1] + zs[-1, -20] + zs[-20, -1] + zs[-20, -20]
facetcol <- cut(zfacet, n.cols)

persp(x=xs, y=ys, z=zs, theta=-40, phi=30, ticktype='detailed', 
            xlab="CRuns", ylab="RBI", zlab="Salary", 
            col = palette[facetcol])
```

### Referências

Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.

Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.
