---
title: "Regressão logística"
author: "Robson Bruno Dutra Pereira"
format: html
editor: visual
---

## Regressão logística

### Regressão logística binária simples

Seja uma variável categórica com duas categorias, $y \in \{c_1,c_2\}$, dita binária ou dicotômica, a qual suspeita-se ser dependente de uma única variável contínua ou real, $x$, $x \in \mathcal{R}$. Como exemplo tem-se respostas com categorias "sim" e "não" ou "sucesso" e "fracasso". Um modelo de regressão logística visa prever a probabilidade de sucesso dado um valor de x, isto é $p(y=sucesso|x)$. O sucesso deve ser entendido não literalmente, mas como a categoria de mais interesse a ser prevista. Obviamente a remanescente terá probabilidade complementar.

Considere o caso onde deseja-se prever se a pessoa compra ou não um iphone de última geração tomando como variável independente o seu salário. Tais categorias são codificadas respectivamente em 0 e 1.

```{r}
#| message: false
#| warning: false
#| echo: false

library(ggplot2)

set.seed(7)
salario <- runif(50, 1000, 20000) 
s <- order(salario)
salario <- salario[s]

compra <- c(rep(0,25),rep(1,25))
compra[20:30] <- sample(0:1, 11, replace = T)

iphone <- data.frame(x=salario,
                     y=compra,
                     classe=ifelse(compra==0, "não", "sim"))

ggplot(iphone, aes(x,y, group=compra)) +
  geom_point(aes(color=classe, pch=classe)) +
  labs(color="compra?",pch="compra?",
       x="salário [R$]", y = "p(y = 1)") +
  theme_bw()
```

Inicialmente alguém poderia pensar em estimar uma reta via mínimos quadrados para prever a probabilidade do indivíduo comprar o iphone segundo o seu salário, isto é: 

$$
p(y=1|x) =\beta_0+\beta_1x
$$ 
Tal procedimento resultaria na regressão plotada abaixo. O problema é que a reta obtida prevê probabilidades negativas para salários menores que R\$3200 reais e probabilidades maiores que 1 para salários maiores que R\$19300. Tais previsões são irreais, uma vez que $p(y) \in [0,1]$.

```{r}
#| message: false
#| warning: false
#| echo: false

ggplot(iphone, aes(x,y)) +
  geom_smooth(method="lm", se=F, col = "black") + 
  geom_point(aes(color=classe, pch=classe)) +
  labs(color="compra?",pch="compra?",
       x="salário [R$]", y = "p(y = 1)") +
  theme_bw()
```

Para garantir previsões no domínio da probabilidade, podem ser aplicadas diversas funções. A função logit ou sigmoid é uma delas. Estas função é definida matematicamente abaixo e plotada a seguir.

$$
p(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{e^z+1}
$$

```{r}
#| message: false
#| warning: false
#| echo: false

z <- seq(-5, 5, by = 0.1)
p <- 1 / (1 + exp(-z))

data <- data.frame(z,p)

ggplot(data, aes(z,p)) +
  geom_line(size=1) +
  theme_bw()
```


Aplicando um modelo linear em função da variável independente em tal função, isto é, fazendo $z=\beta_0+\beta_1x$, obtém-se o modelo de regressão logística, conforme segue.

$$
p(y=1|x) = \frac{1}{1 + e^{-(\beta_0+\beta_1x)}}
$$

O ajuste de um modelo de regressão logística para o exemplo do iphone resulta na função plotada a seguir.

```{r}
#| message: false
#| warning: false
#| echo: false

ggplot(iphone, aes(x,y)) +
  geom_smooth(method="glm", method.args = list(family = "binomial"), se=F, col = "black") + 
  geom_point(aes(color=classe, pch=classe)) +
  labs(color="compra?",pch="compra?",
       x="salário [R$]", y = "p(y = 1)") +
  theme_bw()
```

Sabendo que a probabilidade da outra classe é o complementar, isto é, $p(y=0|x)=1-p(y=1|x)$, pode-se obter o inverso da função logit, ou mais diretamente, da função de regressão logística, conforme segue.

$$
\begin{matrix}
p(y=0|x)=1-p(y=1|x) \\
p(y=0|x)=1-\frac{1}{1 + e^{-(\beta_0+\beta_1x)}} \\
p(y=0|x)=\frac{1 + e^{-(\beta_0+\beta_1x)}-1}{1 + e^{-(\beta_0+\beta_1x)}} \\
p(y=0|x)=\frac{e^{-(\beta_0+\beta_1x)}}{1 + e^{-(\beta_0+\beta_1x)}} \\
\end{matrix}
$$

Tomando a razão entre $p(y=1|x)$ e $1-p(y=1|x)$ ou $p(y=0|x)$, chamada de razão de chance ou *odds ratio*, tem-se.

$$
\frac{p(y=1|x)}{1-p(y=1|x)} = e^{\beta_0+\beta_1x}
$$

Tal razão tem domínio entre 0 e $\infty$, com uma razão unitária indicando chance igual de ocorrência de ambos grupos, uma razão maior que 1 indicando maior chance de ocorrência do grupo 1 e uma razão menor que 1 indicando maior probabilidade de ocorrência do grupo 0. 
Aplicando logaritmo em ambos os lados do último resultado, verifica-se que o modelo de regressão logística (entenda-se aqui log como o logaritmo neperiano), obtido a partir da aplicação da função logit, pode ser concebido como um modelo de regressão linear simples para o log da razão de chance. O lado esquerdo da relação é chamado de *log-odds* ou logit, que viabiliza a obtenção da subtração das probabilidades em escala log, $\text{log }\frac{p}{(1-p)} = \text{log } p - \text{log }(1-p)$.

$$
\text{log } \biggl(\frac{p(y=1|x)}{1-p(y=1|x)}\biggr) = \beta_0+\beta_1x
$$

### Estimativa do modelo de regressão logística

A estimativa dos coeficientes do modelo de regressão é realizada pela maximização da função de verossimilhança. Para simplificar a notação, considere $p(y_i|x_i,\beta_0,\beta_1)=p(x_i)$.

$$
\begin{matrix}
l(\beta_0,\beta_1) = \text{log } \prod_{i=1}^Np(x_i) \\
l(\beta_0,\beta_1) = \text{log } \prod_{i=1}^N p(x_i)^{y_i}\bigl[1-p(x_i)\bigr]^{1-y_i}\\
l(\beta_0,\beta_1) = \sum_{i=1}^N \text{log } \bigl\{ p(x_i)^{y_i}\bigl[1-p(x_i)\bigr]^{1-y_i} \bigr\} \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i\text{log }  p(x_i) + (1-y_i)\text{log }\bigl[1-p(x_i)\bigr] \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i\text{log } \Bigl( \frac{p(x_i)}{1-p(x_i)} \Bigr) + \text{log }\bigl[1-p(x_i)\bigr] \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i(\beta_0+\beta_1x_i) + \text{log } \Bigl(1- \frac{1}{1+e^{-(\beta_0+\beta_1x_i)}} \Bigr) \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i(\beta_0+\beta_1x_i) + \text{log } \Bigl( \frac{e^{-(\beta_0+\beta_1x_i)}}{1+e^{-(\beta_0+\beta_1x_i)}} \times \frac{e^{(\beta_0+\beta_1x_i)}}{e^{(\beta_0+\beta_1x_i)}} \Bigr) \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i(\beta_0+\beta_1x_i) + \text{log } \Bigl( \frac{1}{1+ e^{(\beta_0+\beta_1x_i)}} \Bigr) \\
l(\beta_0,\beta_1) = \sum_{i=1}^N y_i(\beta_0+\beta_1x_i) - \text{log } \bigl( 1+ e^{(\beta_0+\beta_1x_i)} \bigr) \\
\end{matrix}
$$

Considerando o modelo de regressão logística em notação matricial, podemos reescrever a função do log da verossimilhança, conforme segue.

$$
l(\beta) = \sum_{i=1}^N y_i(\beta^T\mathbf{x}_i) - \text{log } \bigl( 1+ e^{(\beta^T\mathbf{x}_i)} \bigr) \\
$$

O vetor $\beta$ considera ambos os coeficientes $\beta = [\beta_0,\beta_1]^T$ enquanto o vetor $\mathbf{x}_i$ considera a constante e a observação da variável regressora, $\mathbf{x}_i = [1,x]^T$. Para maximizar o log da verossimilhança, igualamos a derivada da função a zero, isto é:

$$
\begin{matrix}
\frac{\partial l(\beta)}{\partial \beta} = \frac{\partial }{\partial \beta} \sum_{i=1}^N y_i(\beta^T\mathbf{x}_i) - \text{log } \bigl( 1+ e^{(\beta^T\mathbf{x}_i)} \bigr) \\
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N y_i\mathbf{x}_i - \frac{e^{\beta^T\mathbf{x}_i}}{1+e^{\beta^T\mathbf{x}_i}}\mathbf{x}_i \\
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N y_i\mathbf{x}_i - \frac{1}{1+e^{-\beta^T\mathbf{x}_i}}\mathbf{x}_i \\
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N y_i\mathbf{x}_i - p(\mathbf{x}_i,\beta)\mathbf{x}_i \\
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N \mathbf{x}_i(y_i - p(\mathbf{x}_i,\beta)) = 0 \\
\end{matrix}
$$

Tal resultado consite em um sistema de duas equações não lineares para o caso simples. Para resolver este problema pode-se usar o método de Newton-Raphson. Este requer além do gradiente a derivada segunda ou a Hessiana da função do log da verossimilhança, conforme segue.

$$
\frac{\partial^2 l(\beta)}{\partial \beta \partial\beta^T} = \sum_{i=1}^N \mathbf{x}_i\mathbf{x}_i^Tp(\mathbf{x}_i,\beta)(y_i - p(\mathbf{x}_i,\beta))
$$

Uma atualização de $\beta$ é obtida conforme segue:

$$
\beta_{t+1}=\beta_t - \Biggl(\frac{\partial^2 l(\beta)}{\partial \beta \partial\beta^T}\Biggr)^{-1}\frac{\partial l(\beta)}{\partial \beta}
$$

Considere o caso onde deseja-se prever se uma pessoa tem ou não diabetes segundo o nível de glicose. Considere as primeiras linhas de um conjunto de dados com observações de 763 pacientes.

```{r}
#| message: false
#| warning: false
#| echo: false
library(gt)
library(mlbench)
library(dplyr)

data(PimaIndiansDiabetes2)
# head(PimaIndiansDiabetes2)

data <- PimaIndiansDiabetes2 |>
  select(glucose,diabetes)

head(data) |> gt()
```

```{r}
#| message: false
#| warning: false
#| echo: false

# Filtrar dados para remover NA
data <- data %>% filter(!is.na(glucose), !is.na(diabetes))

# Transformar a variável resposta em binária
data$diabetes <- ifelse(data$diabetes == "pos", 1, 0)

# Dividindo dados em treinamento e teste
set.seed(7)
treino <- sample(nrow(data), 0.75*nrow(data))
dados_treino <- data[treino,]
dados_test <- data[-treino,]
```

Seja um modelo de regressão logística para tal caso considerando como observações de treino 75\% das observações disponíveis tomadas aleatoriamente.

```{r}
#| message: false
#| warning: false
#| echo: false
model1 <- glm(diabetes ~ glucose, data = dados_treino, family = binomial)
summary(model1)
```
O coeficiente relacionado ao nível de glicose apresenta significância estatística. No caso da regressão logística os coeficientes do modelo não podem ser diretamente interpretados considerando a probabilidade prevista, mas sim o log da razão de chance, de forma que neste caso apresentam interpretação similar aos coeficientes de regressão linear simples. Tal modelo pode ser plotado, conforme segue.

```{r}
#| message: false
#| warning: false
#| echo: false
ggplot(dados_treino, aes(glucose, diabetes)) +
  geom_point(aes(color = as.factor(diabetes)), alpha = 0.1) +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = F) +
  labs(x = "Concentracao de glicose", 
       y = "Probabilidade de ter diabetes",
       color = "diabetes?") +
  theme_bw()
```


```{r}
#| message: false
#| warning: false
#| echo: false
#| output: false
# Função para o algoritmo de Newton-Raphson com armazenamento dos betas
newton_raphson <- function(X, y, max_iter = 100, tol = 1e-6) {
  n <- nrow(X)
  p <- ncol(X) + 1  # +1 para o intercepto
  
  # Inicializar os coeficientes
  beta <- rep(0, p)
  
  # Adicionar uma coluna de 1s para o intercepto
  X <- cbind(1, X)
  
  # Armazenar os betas em cada iteração
  beta_history <- matrix(0, nrow = max_iter + 1, ncol = p)
  beta_history[1, ] <- beta
  
  for (iter in 1:max_iter) {
    # Calcular a probabilidade prevista
    p_hat <- 1 / (1 + exp(-X %*% beta))
    
    # Calcular o vetor de erros
    error <- y - p_hat
    
    # Calcular a matriz Hessiana
    W <- diag(as.vector(p_hat * (1 - p_hat)), n, n)
    H <- t(X) %*% W %*% X
    
    # Calcular o gradiente
    gradient <- t(X) %*% error
    
    # Atualizar os coeficientes
    beta_new <- beta + solve(H) %*% gradient
    
    # Armazenar os betas
    beta_history[iter + 1, ] <- beta_new
    
    # Verificar a convergência
    if (sum(abs(beta_new - beta)) < tol) {
      cat("Convergência alcançada após", iter, "iterações.\n")
      beta_history <- beta_history[1:(iter + 1), ]
      break
    }
    
    beta <- beta_new
  }
  
  return(beta_history)
}

```

```{r}
#| message: false
#| warning: false
#| echo: false
#| output: false
# Preparar os dados
X <- as.matrix(dados_treino$glucose)
y <- dados_treino$diabetes

# Aplicar o algoritmo de Newton-Raphson
beta_history <- newton_raphson(X, y)

# Plotar as curvas de contorno com as atualizações de beta
beta0 <- beta_history[, 1]
beta1 <- beta_history[, 2]

# Gerar a grade de valores para o gráfico
beta0_seq <- seq(min(beta0) - 1, max(beta0) + 1, length.out = 100)
beta1_seq <- seq(min(beta1) - 1, max(beta1) + 1, length.out = 100)
log_likelihood <- matrix(0, nrow = length(beta0_seq), ncol = length(beta1_seq))

for (i in 1:length(beta0_seq)) {
  for (j in 1:length(beta1_seq)) {
    b <- c(beta0_seq[i], beta1_seq[j])
    p_hat <- 1 / (1 + exp(-(b[1] + X * b[2])))
    log_likelihood[i, j] <- sum(y * log(p_hat) + (1 - y) * log(1 - p_hat))
  }
}

# Transformar a matriz de log-likelihood em formato long para ggplot
log_likelihood_df <- expand.grid(beta0 = beta0_seq, beta1 = beta1_seq)
log_likelihood_df$log_likelihood <- as.vector(log_likelihood)

beta_history[nrow(beta_history),]
```

```{r}
#| message: false
#| warning: false
#| echo: false
#| output: false
contour(beta0_seq, beta1_seq, log_likelihood, 
        xlab = "Intercepto (beta0)", 
        ylab = "Coeficiente de glucose (beta1)" 
        # main = "Curvas de Contorno com Atualização de Beta"
        )
points(beta0, beta1, col = "red", pch = 19)
lines(beta0, beta1, col = "blue")
```

O modelo obtido no exemplo pode ser escrito conforme segue. Tal modelo é útil para prever a probabilidade de o indivíduo ter diabetes em função do seu nível de glicose.

$$
p(y=1|x) = \frac{1}{1 + e^{-(-5.712+0.040x)}}
$$

### Regressão logística binária múltipla

Seja o caso onde deseja-se estudar a probabilidade de sucesso de uma variável de resposta categórica binária em função de múltiplos preditores, $x_1,x_2,\ldots,x_k$, $p(y=1|x_1,x_2,\ldots,x_k)$. 

Um modelo de regressão logística múltipla pode ser escrito conforme segue.

$$
p(y=1|x) = \frac{1}{1 + e^{-(\beta_0+\beta_1x_1+\beta_2x_2+\ldots+\beta_kx_k)}}
$$

Seja a matrix de observações das variáveis preditoras $\mathbf{X}_{[N\times (k+1)]}$, com $\mathbf{\beta}_{[(k+1) \times 1]}$ como o vetor de coeficientes, $\mathbf{y}_{[N]}$ o vetor de observações da resposta binária, $\mathbf{p}_{[N]}$ o vetor de probabilidades ajustadas com i-ésimo elemento $p(\mathbf{x}_i,\beta_{t})$ e $\mathbf{W}_{[N\times N]}$ uma matriz diagonal de pesos com o i-ésimo elemento diagonal igual a $p(\mathbf{x}_i,\beta_{t})(1-p(\mathbf{x}_i,\beta_{t}))$. Temos o gradiente e a hessina tomando tal notação conforme segue.

$$
\frac{\partial l(\beta)}{\partial \beta} = \mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\
$$
$$
\frac{\partial^2 l(\beta)}{\partial \beta \partial\beta^T} = -\mathbf{X}^T\mathbf{W}\mathbf{X}
$$

A atualização no método de Newton-Raphson para $\beta$ fica conforme segue:

$$
\beta_{t+1}=\beta_t - (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{p})
$$

Considerando ainda o caso onde deseja-se prever se a pessoa tem ou não diabetes, porém, não apenas em função do nível de glicose no sangue, mas também em função da idade. 

```{r}
#| message: false
#| warning: false
#| echo: false
data <- PimaIndiansDiabetes2 |>
  select(glucose,diabetes, age)

head(data) |> gt()
```

```{r}
#| message: false
#| warning: false
#| echo: false

# Filtrar dados para remover NA
data <- data %>% filter(!is.na(glucose), !is.na(diabetes))

# Transformar a variável resposta em binária
data$diabetes <- ifelse(data$diabetes == "pos", 1, 0)

# Dividindo dados em treinamento e teste
set.seed(7)
treino <- sample(nrow(data), 0.75*nrow(data))
dados_treino <- data[treino,]
dados_test <- data[-treino,]
```

Seja um modelo de regressão logística para tal caso, tomando novamente 75\% das observações disponíveis parta treino. No caso múltiplo é importante padronizar as variáveis.

```{r}
#| message: false
#| warning: false
#| echo: false
model2 <- glm(diabetes ~ scale(glucose) + scale(age), data = dados_treino, family = binomial)
summary(model2)
```

Pode-se observar que o coeficiente para a idade é significativo, porém não tanto quanto o coeficiente do nível de glicose. 

O modelo de regressão logística apresenta pressuposiçoes similares ao modelo de regressão linear, tais como: independência das observações, linearidade dos dados, homogeneidade dos erros, independência dos erros e ausência de multicolinearidade. Entretanto, como este curso é mais voltado para previsão e menos para inferência, será dada mais ênfase nas métricas de previsão para avaliar a capacidade de generalização do modelo.

### Avaliação da capacidade de previsão do modelo de regressão

Conforme já discutido, o modelo de regressão logística é útil para prever a probabilidade de ocorrência de uma classe de interesse. No entanto,em aprendizado deseja-se classificar observações futuras. Para usar um modelo de regressão logística para tal fim deve-se discretizar a probabilidade considerando um valor de corte de interesse, geralmente $p=0,5$. 

$$
\biggl\{
\begin{matrix}
\hat{y}=1\text{, se }  p(y=1|\mathbf{x}) \geq 0,5 \\
\hat{y}=0\text{, se }  p(y=1|\mathbf{x}) < 0,5 \\
\end{matrix}
$$

A partir de tal discretização é possível obter a matriz de confusão, que resume todas as possíveis combinações de classificações considerando a realidade e o modelo.

```{r}
#| message: false
#| warning: false
#| echo: false

confmat <- data.frame(`verdade/previsão` = c("verdade/previsão", "classe 0", "classe 1"),
  `classe 0` = c("classe 0", "verdadeiro negativo", "falso negativo"),
`classe 1` = c("classe 1", "falso positivo", "verdadeiro positivo"))
confmat |> gt() |>
  tab_options(column_labels.hidden = TRUE)
```


A matriz de confusão a seguir apresenta os resultados do modelo de regressão logística aplicados aos 25\% dos dados separados para teste para o exemplo de classificação de diabéticos. Pode-se observar que de 120 pessoas sem diabetes, $y=0$, o modelo classifica corretamente 105 pessoas, resultando em 15 falsos positivos, enquanto de 71 pessoas com diabetes, o modelo classifica corretamente 36 e resulta em 35 falsos negativos. 

```{r}
#| message: false
#| warning: false
#| echo: false

prob_pred2 <- predict(model2, type = 'response', newdata = dados_test)
y_pred2 <- ifelse(prob_pred2 > 0.5, 1, 0)

cm2 <- table(y = dados_test$diabetes, pred = y_pred2)
cm2
```

A partir da matriz de confusão é possível calcular a acuracidade do modelo. De forma geral a acuracidade do modelo pode ser expressa considerando a proporção de acerto, conforme segue, onde TP é o número de verdadeiros positivos (*true positive*), TN é o número de verdadeiros negativos (*true negative*), FP é o número de falsos positivos (*false positive*), FN é o número de false negativos (*false negative*). De forma geral $I(\hat{y}=y)$ é uma função indicativa que retorna 1, se veraddeira, sendo que a soma $\sum I(\hat{y}=y)$ considera o total de classificações corretas, $TP+TN$, sendo n o total de observações de teste. Para o exemplo a acuracidade é de 73,82\%.

$$
Acc = \frac{TP+TN}{TP+TN+FP+FN} = \frac{\sum I(\hat{y}=y)}{n}
$$

Além de ser possível constatar a falta de equilíbrio entre as duas classes, observa-se um alto número de indivíduos com diabetes classificados erroneamente. O desbalanceamento entre as classes, possivelmente também presente nos dados de treino, tem consequência na diferença de proporção entre falsos positivos e negativos. Devido ao desequilíbrio entre classes a acuracidade geral pode não ser a melhor métrica para avaliar a capacidade do modelo. O gráfico a seguir apresenta o modelo de regressão logística múltipla obtido.

```{r}
#| message: false
#| warning: false
#| echo: false
grid <- expand.grid(glucose = seq(min(dados_treino$glucose), 
                                  max(dados_treino$glucose), length=200),
                    age = seq(min(dados_treino$age), 
                                  max(dados_treino$age), length=200))

prob_grid <- predict(model2, type = 'response', newdata = grid)
y_pred_grid <- ifelse(prob_grid > 0.5, 1, 0)

grid$diabetes <- as.factor(y_pred_grid)

ggplot() + 
  geom_raster(aes(x = grid$glucose, y = grid$age, fill = grid$diabetes), 
              alpha = .5) +
  geom_point(aes(x = dados_test$glucose, y = dados_test$age, 
                 color = as.factor(dados_test$diabetes)), size = 2) + 
  labs(x = "Nivel de glicose", y = "Idade", 
       fill = "Diabetes", col = "Diabetes") + theme_bw()
```

Consideremos outras variáveis regressoras no modelo de regressão logística.

```{r}
#| message: false
#| warning: false
#| echo: false

library(tidyr)

data <- PimaIndiansDiabetes2 

# Filtrar dados para remover NA
data <- data |> 
  drop_na()

head(data) |> gt()
```

O modelo obtido foi reduzido com eliminação para trás. Pode-se observar que as variáveis massa e pedigree também contribuem para melhorar o modelo, apesar de apenas a primeira variável adicional ser significativa a 0,05, além das consideradas inicialmente.

```{r}
#| message: false
#| warning: false
#| echo: false

data <- data |>
  mutate(across(where(is.numeric), scale))

data$diabetes <- ifelse(data$diabetes == "pos", 1, 0)

set.seed(7)
treino <- sample(nrow(data), 0.75*nrow(data))
dados_treino <- data[treino,]
dados_test <- data[-treino,]

model3 <- glm(diabetes ~ ., data = dados_treino, family = binomial)
model3 <- step(model3, trace=F)

summary(model3)
```

A matriz de confusão obtida é apresentada a seguir. A acuracidade para este modelo é de 82,65\%. 

```{r}
#| message: false
#| warning: false
#| echo: false
prob_pred3 <- predict(model3, type = 'response', newdata = dados_test)
y_pred3 <- ifelse(prob_pred3 > 0.5, 1, 0)

cm3 <- table(y = dados_test$diabetes, pred = y_pred3)
cm3
```

Em problemas de classificação da área de saúde a sensitividade a especificidade são usadas, sendo a primeira  a probabilidade de prever a doença dado que a pessoa é de fato doente, enquanto a especificidade consiste na probabilidade de prever como não doente dado que a pessoa de fato não o é. Para o exemplo estudado, considerando o último modelo obtido, tem-se uma sensibilidade igual a 62,5\% e uma especificidade igual a 92,42\%.

$$
\biggl\{
\begin{matrix}
Sens = \frac{TP}{TP+FN} \\
Spec = \frac{TN}{TN+FP}
\end{matrix}
$$

A curva ROC (*receiver operating characteristics*) mede graficamente a relação entre tais medidas e é plotada para o exemplo a seguir. Pode-se observar que para obter uma especificidade igual a 92\% tem-se 62.5\% de sensitividade.

```{r}
#| message: false
#| warning: false
#| echo: false

library(pROC)

rocobj <- roc(dados_test$diabetes, y_pred3)
ggroc(rocobj) + theme_bw()
```

A área abaixo da curva ROC também é utilizada como métrica de capacidade de previsão. Para o exemplo a curva apresentada tem área igual a 0,775 abaixo dela.

### Regressão logística multinomial

Considere o caso onde a variável dependente apresenta mais de duas classes, $y \in \{c_1,c_2,\ldots,c_Q\}$, $Q>2$. Um modelo de regressão logística multinomial simples pode ser escrito conforme segue.

$$
\begin{matrix}
\text{log } \biggl(\frac{p(y=1|x)}{p(y=Q|x)}\biggr) = \beta_{10}+\beta_1x \\
\text{log } \biggl(\frac{p(y=2|x)}{p(y=Q|x)}\biggr) = \beta_{20}+\beta_2x \\
\vdots \\
\text{log } \biggl(\frac{p(y=Q-1|x)}{p(y=Q|x)}\biggr) = \beta_{(Q-1)0}+\beta_{(Q-1)}x \\
\end{matrix}
$$

O modelo foi especificado considerando $Q-1$ transformações logit. A classe considerada no denominador é definida arbitrariamente. De forma análoga ao modelo binário, tem-se:

$$
\begin{matrix}
p(y=q|x) = \frac{1}{1 + \sum_{l=1}^{Q-1} e^{-(\beta_{q0}+\beta_qx)}} \text{, } q=1,\ldots,Q-1 \\
p(y=Q|x)=\frac{1}{1 + \sum_{l=1}^{Q-1} e^{\beta_{q0}+\beta_qx}} \\
\end{matrix}
$$

O modelo de regressão logística multinomial simples pode ser obviamente ser estendido ao caso múltiplo.

Assim como na regressão linear para respostas contínuas a regressão logística também permite a aplicação de técnicas de regularização como regressão logística rígida e LASSO. Ademais, é possível cnsiderar termos polinomiais e de interação e até mesmo expansões de base, como *splines* e GAM.