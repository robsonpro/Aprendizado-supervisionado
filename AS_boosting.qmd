---
title: "Reforço de gradiente"
author: "Robson Bruno Dutra Pereira"
format: html
editor: visual
---

## Máquinas de reforço de gradiente

As máquinas de reforço de gradiente (*gradient boosting machines* - GBM) foram propostas por Gerome Friedman e consistem em um método de aprendizado por reforço que visa aproximar o gradiente da função perda, ou de forma mais simples, visa aproximar o erro de previsão e diminuir este a cada iteração do método. O conceito de aprendizado por reforço está relacionado ao fato de o método de certa forma aprender através do erro, uma vez que vai melhorando a previsão a cada tentativa ou iteração a partir dos valores residuais ou de erro observados. O método foi proposto inicialmente considerando árvores de decisão ou regressão, mas poderia ser utilizado também no caso de regressão por mínimos quadrados.

Um algoritmo GBM simplista do método GBM para regressão é apresentado à seguir:

1.  Defina $\hat{f}(\mathbf{x})=0$ e $\varepsilon_i = y_i$ para todos dados de treino.
2.  Para $m=1,...,M$:

<!-- -->

i)  Estime uma árvore, $\hat{f}_m$, para os dados de treino $(\mathbf{x}_i,\varepsilon_i)$, $i=1,...,N$;
ii) Atualize $\hat{f}(\mathbf{x})$ adicionando a árvore de regressão obtida multiplicada por uma taxa de aprendizagem $\lambda$, isto é, $\hat{f}(\mathbf{x}) \leftarrow \hat{f}(\mathbf{x}) + \lambda\hat{f}_m(\mathbf{x})$;
iii) Atualize os resíduos, isto é $\varepsilon_i \leftarrow \varepsilon_i - \lambda\hat{f}_m(\mathbf{x}_i)$, $i=1,...,N$;

<!-- -->

3.  Atualize o modelo "impulsionado" ou "reforçado": $\hat{f}(\mathbf{x}) = \sum_{m=1}^M\lambda\hat{f}_m(\mathbf{x})$

O número de iterações ou árvores consideradas, $M$, pode ser definido por validação cruzada. A taxa de aprendizado, $\lambda$, geralmente é um valor real muito pequeno, por exemplo 0,01 ou 0,001 e define a taxa que o modelo por refroço aprende, isto é, melhora sua previsão. Quanto menor $\lambda$, maior o número de árvores, $M$, necessário para uma boa aproximação.

Geralmente $\hat{f}_m(\mathbf{x})$ é um modelo de árvore com apenas uma partição e portanto dois nós terminais ou regiões, sendo portanto um aprendiz fraco (*weak leaner*). Em alguns casos pode ser interessante modelos com árvores com duas partições, o que de certa forma implica em modelos com interação.

Considerando a função perda quadrática comumente usada em problemas de regressão:

$$
\begin{aligned}
L = \frac{1}{2}(y - \hat{f}(\mathbf{x}))^2
\end{aligned}
$$

A divisão por 2 viabiliza facilitar os cálculos no algoritmo de *boosting*. Tomando a derivada de $L$ em relação a $f$ obtém-se o valor residual ou erro em relação à resposta de interesse, isto é:

$$
\begin{aligned}
\frac{\partial L}{\partial f} = y - \hat{f}(\mathbf{x}) = \varepsilon
\end{aligned}
$$

Portanto, em cada iteração, o algoritmo GBM, ao aproximar uma árvore considerando $N$ observações das variáveis regressoras, $\mathbf{x}_i$, em relação à $N$ valores residuais, $\varepsilon_i$, $i=1,...,N$, aproxima o gradiente da função perda que, no caso da função perda quadrática, é o valor residual ou erro, $\varepsilon$.

Para entender melhor o algoritmo GBM, considere o conjunto arbitrário de dados de treino a seguir, com duas variáveis independentes, $x_1$ e $x_2$, e uma dependente contínua, $y$.

```{r}
#| message: false
#| warning: false
#| echo: false
library(gt)

set.seed(8)
x1 <- rnorm(10)
x2 <- rnorm(10, 2)
y <- 0.3*x1+sqrt(x2)+rnorm(10,1,0.2)

dat <- round(data.frame(x1,x2,y),2)

dat |> gt()
```

Consideremos para fins didáticos uma taxa de aprendizagem alta, $\lambda$ = 0.5, e um número pequeno de iterações, $M = 4$ árvores. O algoritmo inicia fazendo $\hat{f}(x_1,x_2)=0$ e $\varepsilon_i = y_i$, isto é (a penúltima coluna seriam os valores previstos do modelo inicial, $\hat{f} = f$, e a última coluna seria o erro, $\varepsilon = e$):

```{r}
#| message: false
#| warning: false
#| echo: false

dat$f <- 0
dat$e <- dat$y

lambda <- 0.5

dat |> gt()
```

Fazendo $m=1$, estima-se a primeira árvore, $\hat{f} = T$, para os resíduos em função das variáveis independentes, $(x_1,x_2,\varepsilon)_i$, $i=1,...,N$.

```{r}
#| message: false
#| warning: false
#| echo: false
library(tree)
tree1 <- tree(e ~ x1+x2, dat)
tree1
```

O modelo obtido é usado para atualizar os valores previstos, $\hat{f}(x_1,x_2)\leftarrow \hat{f}(x_1,x_2) + \lambda\hat{f}_1(x_1,x_2)$ e os resíduos, $\varepsilon_i \leftarrow \varepsilon_i - \lambda\hat{f}_1(x_1,x_2)$.

```{r}
#| message: false
#| warning: false
#| echo: false

pred1 <- predict(tree1, dat)

dat$f <- dat$f + lambda*pred1

dat$e <- dat$e -lambda*pred1

dat |> gt()
```

Tomando $m=2$, repete-se o processo, obtendo-se uma nova árvore para os resíduos e atualizando as previsões e os resíduos.

```{r}
#| message: false
#| warning: false
#| echo: false
tree2 <- tree(e ~ x1+x2, dat)
tree2
```

```{r}
#| message: false
#| warning: false
#| echo: false
pred2 <- predict(tree2, dat)

dat$f <- dat$f + lambda*pred2

dat$e <- dat$e -lambda*pred2

dat |> gt()
```

Para $m=3$, tem-se:

```{r}
#| message: false
#| warning: false
#| echo: false
tree3 <- tree(e ~ x1+x2, dat)
tree3
```

```{r}
#| message: false
#| warning: false
#| echo: false
pred3 <- predict(tree3, dat)

dat$f <- dat$f + lambda*pred3

dat$e <- dat$e -lambda*pred3

dat |> gt()
```

Finalmente, para $m=4$, obtém-se o modelo "reforçado" final que consiste na soma ponderada pela taxa de aprendizagem de todos os modelos, isto é, $\hat{f}(\mathbf{x}) = \sum_{m=1}^M\lambda\hat{f}_m(\mathbf{x})$.

```{r}
#| message: false
#| warning: false
#| echo: false
tree4 <- tree(e ~ x1+x2, dat)
tree4
```

```{r}
#| message: false
#| warning: false
#| echo: false
pred4 <- predict(tree4, dat)

dat$f <- dat$f + lambda*pred4

dat$e <- dat$e -lambda*pred4

dat |> gt()
```

Obviamente, o processo poderia seguir para melhorar as previsões. Conforme já elucidado, pode-se definir o número de iterações por validação cruzada. Quanto maior o número de árvores estipulado, recomenda-se que menor seja a taxa de aprendizagem.

```{r}
#| message: false
#| warning: false
#| echo: false

library(AER)
data(PSID7682)
# ?PSID7682
dados <- PSID7682
dados$Name <- NULL
dados <- dados[,-c(13:14)]
tr <- round(0.5*nrow(dados))
treino <- sample(nrow(dados), tr, replace = F)
```

```{r}
#| message: false
#| warning: false
#| echo: false


library(gbm)

boost1 <- gbm(wage~., data = dados, distribution = "gaussian", 
              n.trees = 8000, bag.fraction = 0.7, 
              interaction.depth = 2, shrinkage = 0.1)
# summary(boost1)
```

Seja um conjunto de dados para prever o rendimento de indivíduos considerando anos de experiência, ocupação, se ele trabalha ou não na indústria, escolaridade, além de fatores sócio-geográficos. Considerando um modelo obtido via GBM com 8000 árvores e $\lambda = 0,1$, pode-se plotar um gráfico de superfície considerando como variáveis independentes a escolaridade e anos de experiência conforme segue.

```{r}
#| message: false
#| warning: false
#| echo: false
#| fig-width: 8
#| fig-height: 8
# Plotando superficie do modelo boost1
xs <- seq(min(dados$experience), max(dados$experience), length = 40)
ys <- seq(min(dados$education), max(dados$education), length = 40)
xys <- expand.grid(xs,ys)
colnames(xys) <- c("experience", "education")

# fixando nivel das variaveis nao plotadas
xys$weeks <- mean(dados$weeks)
xys$occupation <- "white"
xys$industry <- "yes"
xys$south <- "yes"
xys$smsa <- "yes"
xys$married <- "yes"
xys$gender <- "male"
xys$union <- "no"
xys$ethnicity <- "other"

zs <- matrix(predict(boost1, xys), nrow = length(xs))

n.cols <- 100
palette <- colorRampPalette(c("lightpink", "lightblue"))(n.cols)
zfacet <- zs[-1, -1] + zs[-1, -20] + zs[-20, -1] + zs[-20, -20]
facetcol <- cut(zfacet, n.cols)

p1 <- persp(x=xs, y=ys, z=zs, theta=-60, phi=30, ticktype='detailed', 
            xlab="experiência", ylab="educação", zlab="renda", col = palette[facetcol])

```

### Referências

Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.

Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.
